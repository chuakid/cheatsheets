\documentclass[10pt]{article}
\usepackage[]{multicol}
\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{enumitem}
\setlist{nosep}

\usepackage[margin=10mm]{geometry}

\begin{document}

\subsection*{Axioms of Probability}
\begin{enumerate}
    \item $0 \leq \Pr(A) \leq 1$
    \item $\Pr(S) = 1$
    \item If $A_1, A_2, ...$ are mutually exclusive (disjoint),\\ i.e. $A_i \cap A_j = \emptyset$ when $i \neq j$, then $\Pr(\cup_{i=1}^\infty A_i) = \sum_{i=1}^{\infty}\Pr(A_i)$ \\
          In particular, if events $A$ and $B$ are mutually exclusive, then $\Pr(A\cup B) = \Pr(A) + \Pr(B)$
\end{enumerate}
\subsection*{Properties of Probability}
\begin{enumerate}
    \item $\Pr(\emptyset) = 0$
    \item If $A_1, A_2, ..., A_n$ are mutually exclusive events, then $\Pr(\cup_{i=1}^{n}A_i) = \sum_{i=1}^{n}\Pr(A_i)$
    \item $\Pr(A') = 1 - \Pr(A)$
    \item $\Pr(A) = \Pr(A \cap B) + \Pr(A \cap B')$
    \item $\Pr(A \cup B) = \Pr(A) + \Pr(B) - \Pr(A \cap B)$
    \item $Pr (A \cup B \cup C) = \Pr(A) + \Pr(B) + \Pr(C) - \Pr(A \cap B) - \Pr(B \cap C) - \Pr(A \cap C) + \Pr(A \cap B \cap C)$
\end{enumerate}
\subsection*{Conditional Probability, $P(A\mid B)$}
\begin{itemize}
    \item $\Pr(A\mid B) = \frac{\Pr(A\cap B)}{\Pr(B)}$, if $\Pr(A) \neq 0$
    \item For fixed $A$, $\Pr(B\mid A)$ satisfies the postulates of probability.
    \item False positive: $\Pr(\text{+} \mid \text{condition})$
\end{itemize}
\subsubsection*{Multiplication rule}
\begin{itemize}
    \item $\Pr(A\cap B) = \Pr(A) \Pr(B\mid A) = \Pr(B)\Pr(A\mid B)$, providing $\Pr(A) > 0, \Pr(B) > 0$
    \item $\Pr(A\cap B \cap C) = \Pr(A)\Pr(B\mid A)\Pr(C\mid A\cap B)$
    \item $\Pr(A_1\cap...\cap A_n) = \Pr(A_1)\Pr(A_2\mid A_1)\Pr(A_3 \mid A_1 \cap A_2)...\Pr(A_n\mid A_1\cap ... \cap A_{n - 1})$
\end{itemize}
\subsubsection*{The Law of Total Probability}
\begin{itemize}
    \item Let $A_1,A_2,...,A_n$ be a partition of sample space $S$ (mutually exclusive and exhaustive events s.t. $A_i\cap A_j = \emptyset$ for $i\neq j$ and $\cup_{i=1}^n A_i = S$).
    \item Then $\Pr(B) = \sum_{i=1}^{n}\Pr(B\cap A_i) = \sum_{i=1}^{n}\Pr(A_i)\Pr(B\mid A_i)$
\end{itemize}
\subsubsection*{Bayes' Theorem}
\begin{itemize}
    \item Let $A_1,A_2,...,A_n$ be a partition of $S$
    \item $\Pr(A_k\mid B) = \frac{\Pr(A_k)\Pr(B\mid A_k)}{\sum_{i=1}^{n}\Pr(A_i)\Pr(B\mid A_i)} = \frac{\Pr(A_k)\Pr(B\mid A_k)}{\Pr(B)}$, $k \in [1, n]$
\end{itemize}

\subsection*{Independent Events}
\begin{itemize}
    \item Definition: iff $\Pr(A\cap B) = \Pr(A)\Pr(B)$
\end{itemize}
\subsubsection*{Properties}
\begin{itemize}
    \item Suppose $\Pr(A)>0,\Pr(B)>0$, $A$ and $B$ are independent:
          \begin{itemize}
              \item $\Pr(B\mid A) = \Pr(B)$ and $\Pr(A\mid B) = \Pr(A)$
              \item $A$ and $B$ cannot be mutually exclusive (and vice versa)
          \end{itemize}
    \item The sample space $S$ and $\emptyset$ are independent of any event
    \item If $A \subset B$, then $A$ and $B$ are dependent unless $B = S$
\end{itemize}
\subsubsection*{Theorem}
If $A, B$ are indep, then so are $A$ and $B'$, $A'$ and $B$, $A'$ and $B'$.
\subsubsection*{$n$ Independent Events}
\begin{itemize}
    \item \textbf{Pairwise Independent Events}:\\
          Events $A_1,A_2,...,A_n$ are pairwise indep\\
          iff $\Pr(A_i\cap A_j) = \Pr(A_i)\Pr(A_j)$
    \item \textbf{Mutually Independent}:\\
          Events $A_1,A_2,...,A_n$ are (mutually) independent iff for any subset $\{A_{i_1}, A_{i_2},...,A_{i_k}\}$ of $A_1,A_2,...,A_n$,\\
          $\Pr(A_{i_1}\cap A_{i_2}\cap ...\cap A_{i_k}) = \Pr(A_{i_1})\Pr(A_{i_2})...\Pr(A_{i_k})$
\end{itemize}
\subsubsection*{Remarks}
\begin{itemize}
    \item $A_1,A_2,...,A_n$ are mutually independent $\Leftrightarrow$ for any pair of events $A_j,A_k$ where $j\neq k$, the multiplication rule holds, for any 3 distinct events, the multiplication rule holds, and so on $\Pr(A_1\cap A_2\cap ... \cap A_n) = \Pr(A_1)\Pr(A_2) ...\Pr(A_n)$. In total there are $2^n - n - 1$ different cases.
    \item Mutually indep $\Rightarrow$ pairwise indep (not the converse)
    \item Suppose $A_1,A_2,...,A_n$ are mutually indep events, let $B_i=A_i$ or $A_i'$, $i \in [1, n]$. Then $B_1,B_2,...,B_n$ are also mutually indep events.
\end{itemize}

\subsection*{Discrete Probability Distributions}
\subsubsection*{Discrete R.V.}
Let $X$ be an R.V. If $R_X$ is \textbf{finite or countable infinite}, $X$ is discrete R.V.
\subsubsection*{Probability Function (p.f.) or Probability Mass Function (p.m.f.)}
\begin{itemize}
    \item For a discrete R.V., each value $X$ has a certain probability $f(x)$. Such a function $f(x)$ is called the p.f.
    \item The collection of pairs $(x_i, f(x_i))$ is prob distribution of $X$
    \item The probability of $X=x_i$ denoted by $f(x_i)$ must satisfy:
          \begin{enumerate}
              \item $f(x_i) \geq 0 \forall x_i$
              \item $\sum_{i=1}^{\infty}f(x_i)=1$
          \end{enumerate}
\end{itemize}
\subsection*{Continuous Probability Distributions}
\subsubsection*{Continuous R.V.}
Suppose that $R_X$ is an \textbf{interval or a collection of intervals}, then $X$ is a continuous R.V.
\subsubsection*{Probability Density Function (p.d.f.)}
\begin{itemize}
    \item Let $X$ be a continuous R.V.
    \item p.d.f. $f(x)$ is a function satisfying:
          \begin{enumerate}
              \item $f(x) \geq 0 \;\forall x \in R_X$
              \item $\int_{R_X}^{}f(x)dx = 1$ or $\int_{-\infty}^{\infty}f(x)dx = 1$ as $f(x) = 0 \;\forall x \notin R_X$
              \item $\forall c, d : c < d$ (i.e. $(c, d) \subset R_X$),
                    $\Pr(c\leq X \leq d) = \int_{c}^{d}f(x)dx$
          \end{enumerate}
\end{itemize}
\subsubsection*{Remarks}
\begin{itemize}
    \item $\Pr(c\leq X \leq d) = \int_{c}^{d}f(x)dx$ represents area under the graph of the p.d.f. $f(x)$ between $x=c$ and $x=d$
    \item Let $x_0$ be a fixed value, $\Pr(X=x_0) = 0$
    \item $\leq$ and $<$ can be used interchangeably in a prob statement.
    \item $\Pr(A) = 0$ does not necessarily imply $A = \emptyset$
    \item $R_X \in [a, b] \Rightarrow f(x) = 0 \;\forall x \notin [a, b]$
\end{itemize}
\subsection*{Cumulative Distribution Function (c.d.f.)}
\begin{itemize}
    \item Let $X$ be an R.V., discrete or continuous.
    \item $F(x)$ is a c.d.f. of $X$ where $F(x) = \Pr(X\leq x)$
\end{itemize}
\subsubsection*{c.d.f. for Discrete R.V.}
\begin{itemize}
    \item $F(x) = \sum_{t\leq x}^{}f(t) = \sum_{t\leq x}^{} \Pr(X=t)$
    \item c.d.f. of a discrete R.V. is a step function
    \item $\forall a, b$ s.t. $a\leq b$, $\Pr(a\leq X \leq b) = \Pr(X \leq b) - \Pr(X < a) = F(b) - F(a^-)$ where $a^-$ is the largest possible value of $X$ that is strictly less than $a$
    \item $R_X \subset \mathbb{Z}, a, b \in \mathbb{Z} \Rightarrow$
          \begin{itemize}
              \item $\Pr(a \leq X \leq b) = \Pr(X = a$ or $a+1$ or $...$ or $b) = F(b) - F(a - 1)$
              \item Taking $a = b$, $\Pr(X=a) = F(a) - F(a - 1)$
          \end{itemize}
\end{itemize}
\subsubsection*{c.d.f. for Continuous R.V.}
\begin{itemize}
    \item $F(x) = \int_{-\infty}^{\infty}f(t)dt$
    \item $f(x) = \frac{dF(x)}{dx}$ if the derivative exists
    \item $\Pr(a\leq X \leq b) = \Pr(a < X \leq b) = F(b) - F(a)$
    \item $F(x)$ is a non-decreasing function: $x_1<x_2 \Rightarrow F(x_1) \leq F(x_2)$
    \item $0 \leq F(x) \leq 1$
\end{itemize}


\subsection*{Mean and Variance of an R.V.}
\subsubsection*{Expected Value / Mean / Mathematical Expectation}
\begin{itemize}
    \item \textbf{Discrete:} $E(X) = \mu^{}_X = \sum_i^{} x_i f(x_i) = \sum_x^{} x f(x)$
    \item \textbf{Continuous:} $E(X) = \mu^{}_X = \int_{-\infty}^{\infty}xf(x)dx$
    \item \textbf{Remark}: The expected value exists provided the sum/integral exists
\end{itemize}
\subsubsection*{Expectation of a function of an R.V.}
$\forall g(X)$ with p.f. $f^{}_X(x)$
\begin{itemize}
    \item \textbf{Discrete:} $E[g(X)] = \sum_{x}^{} g(x)f^{}_X (x)$
    \item \textbf{Continuous:} $E[g(X)] = \int_{-\infty}^{\infty} g(x) f_X(x)dx$
    \item Provided the sum/integral exists.
\end{itemize}
\subsubsection*{Variance ($\sigma_X^2 = V(X)$)}
\begin{itemize}
    \item $g(x) = (x - \mu^{}_{X})^2$, Let $X$ be an R.V. with p.f. $f(x)$
    \item $\sigma_X^2 = V(X) = E[(X-\mu^{}_X)^2]$
    \item $E[(X-\mu^{}_X)^2] = \begin{cases}
                  \sum_{x}^{} (x - \mu^{}_X)^2 f_X^{}(x) \text{ if X is continuous} \\
                  \int_{-\infty}^{\infty}(x-\mu_X^{})^2 f_X^{}(X) dx \text{ if X is continuous}
              \end{cases}$
    \item $V(X) \geq 0$, $V(X) = E(X^2) - [E(X)]^2$
    \item \textbf{Standard deviation} = $\sigma_X^{} = \sqrt{V(X)}$
\end{itemize}
\subsection*{Properties of Expectation}
\begin{enumerate}
    \item $E(aX+b) = aE(X) + b$
    \item $V(X) = E(X^2) - [E(X)]^2$
    \item $V(aX+b)=a^2V(X)$
\end{enumerate}

\end{document}
